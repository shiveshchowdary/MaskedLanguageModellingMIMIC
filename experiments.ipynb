{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54dcdd8f-b14f-4d9c-b9ce-0bfb41193d5d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "MAX_LEN = 256\n",
    "batch_size = 32\n",
    "d_model = 50\n",
    "num_heads = 4\n",
    "N = 2\n",
    "num_variables = 18 \n",
    "num_variables += 1 #for no variable embedding while doing padding\n",
    "d_ff = 100\n",
    "epochs = 75\n",
    "learning_rate = 1e-5\n",
    "drop_out = 0.2\n",
    "sinusoidal = False\n",
    "th_val_roc = 0.84\n",
    "th_val_pr = 0.48\n",
    "Uniform = True\n",
    "import torch\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "pd.set_option('future.no_silent_downcasting',True)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from utils import MaskedMimicDataSetInHospitalMortality\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from tqdm import tqdm\n",
    "from normalizer import Normalizer\n",
    "from categorizer import Categorizer\n",
    "\n",
    "train_data_path_inhospital = \"/data/datasets/mimic3_18var/root/in-hospital-mortality/train_listfile.csv\"\n",
    "val_data_path_inhospital = \"/data/datasets/mimic3_18var/root/in-hospital-mortality/val_listfile.csv\"\n",
    "\n",
    "train_data_path_phenotyping = \"/data/datasets/mimic3_18var/root/phenotyping/train_listfile.csv\"\n",
    "val_data_path_phenotyping = \"/data/datasets/mimic3_18var/root/phenotyping/val_listfile.csv\"\n",
    "\n",
    "train_data_path_decompensation = \"/data/datasets/mimic3_18var/root/decompensation/train_listfile.csv\"\n",
    "val_data_path_decompensation = \"/data/datasets/mimic3_18var/root/decompensation/val_listfile.csv\"\n",
    "\n",
    "data_dir_inhospital = \"/data/datasets/mimic3_18var/root/in-hospital-mortality/train/\"\n",
    "data_dir_phenotyping = \"/data/datasets/mimic3_18var/root/phenotyping/train/\"\n",
    "data_dir_decompensation = \"/data/datasets/mimic3_18var/root/decompensation/train/\"\n",
    "\n",
    "import pickle\n",
    "\n",
    "with open('normalizer.pkl', 'rb') as file:\n",
    "    normalizer = pickle.load(file)\n",
    "\n",
    "with open('categorizer.pkl', 'rb') as file:\n",
    "    categorizer = pickle.load(file)\n",
    "    \n",
    "\n",
    "mean_variance = normalizer.mean_var_dict\n",
    "cat_dict = categorizer.category_dict\n",
    "\n",
    "\n",
    "train_ds_inhospital = MaskedMimicDataSetInHospitalMortality(data_dir_inhospital, train_data_path_inhospital, mean_variance, cat_dict, 'training', MAX_LEN)\n",
    "val_ds_inhospital = MaskedMimicDataSetInHospitalMortality(data_dir_inhospital, val_data_path_inhospital, mean_variance, cat_dict, 'validation', MAX_LEN)\n",
    "\n",
    "train_ds_phenotyping = MaskedMimicDataSetInHospitalMortality(data_dir_phenotyping, train_data_path_phenotyping, mean_variance, cat_dict, 'training', MAX_LEN)\n",
    "val_ds_phenotyping = MaskedMimicDataSetInHospitalMortality(data_dir_phenotyping, val_data_path_phenotyping, mean_variance, cat_dict, 'validation', MAX_LEN)\n",
    "\n",
    "train_ds_decompensation = MaskedMimicDataSetInHospitalMortality(data_dir_decompensation, train_data_path_decompensation, mean_variance, cat_dict, 'training', MAX_LEN)\n",
    "val_ds_decompensation = MaskedMimicDataSetInHospitalMortality(data_dir_decompensation, val_data_path_decompensation, mean_variance, cat_dict, 'validation', MAX_LEN)\n",
    "\n",
    "\n",
    "train_dataloader_inhospital = DataLoader(train_ds_inhospital, batch_size = batch_size, shuffle=True)\n",
    "val_dataloader_inhospital = DataLoader(val_ds_inhospital, batch_size = 1)\n",
    "\n",
    "train_dataloader_phenotyping = DataLoader(train_ds_phenotyping, batch_size = batch_size, shuffle=True)\n",
    "val_dataloader_phenotyping = DataLoader(val_ds_phenotyping, batch_size = 1)\n",
    "\n",
    "train_dataloader_decompensation = DataLoader(train_ds_decompensation, batch_size = batch_size, shuffle=True)\n",
    "val_dataloader_decompensation = DataLoader(val_ds_decompensation, batch_size = 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f8f53a-6240-4253-a0ef-deb276127102",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch.nn import functional as F\n",
    "import math\n",
    "\n",
    "def t2v(tau, f, out_features, w, b, w0, b0, arg=None):\n",
    "    if arg:\n",
    "        v1 = f(torch.matmul(tau, w) + b, arg)\n",
    "    else:\n",
    "        v1 = f(torch.matmul(tau, w) + b)\n",
    "    v2 = torch.matmul(tau, w0) + b0\n",
    "    return torch.cat([v1, v2], -1)\n",
    "\n",
    "class SineActivation(nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(SineActivation, self).__init__()\n",
    "        self.out_features = out_features\n",
    "        self.w0 = nn.parameter.Parameter(torch.randn(in_features, 1))\n",
    "        self.b0 = nn.parameter.Parameter(torch.randn(1))\n",
    "        self.w = nn.parameter.Parameter(torch.randn(in_features, out_features-1))\n",
    "        self.b = nn.parameter.Parameter(torch.randn(out_features-1))\n",
    "        self.f = torch.sin\n",
    "\n",
    "    def forward(self, tau):\n",
    "        return t2v(tau.unsqueeze(-1), self.f, self.out_features, self.w, self.b, self.w0, self.b0)\n",
    "\n",
    "class CosineActivation(nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(CosineActivation, self).__init__()\n",
    "        self.out_features = out_features\n",
    "        self.w0 = nn.parameter.Parameter(torch.randn(in_features, 1))\n",
    "        self.b0 = nn.parameter.Parameter(torch.randn(1))\n",
    "        self.w = nn.parameter.Parameter(torch.randn(in_features, out_features-1))\n",
    "        self.b = nn.parameter.Parameter(torch.randn(out_features-1))\n",
    "        self.f = torch.cos\n",
    "\n",
    "    def forward(self, tau):\n",
    "        return t2v(tau.unsqueeze(-1), self.f, self.out_features, self.w, self.b, self.w0, self.b0)\n",
    "\n",
    "\n",
    "class ContinuousValueEmbedding(nn.Module):\n",
    "    def __init__(self, d_model):\n",
    "        super().__init__()\n",
    "        self.W = nn.Linear(1, d_model*2)\n",
    "        self.U = nn.Linear(d_model*2, d_model)\n",
    "        self.tanh = nn.Tanh()\n",
    "    def forward(self, x):\n",
    "        out = self.W(x.unsqueeze(2))\n",
    "        out = self.tanh(out)\n",
    "        out = self.U(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class VariableEmbedding(nn.Module):\n",
    "    def __init__(self, d_model, num_variables):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(num_variables+1, d_model)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.embedding(x)\n",
    "    \n",
    "    \n",
    "\n",
    "class Embedding(nn.Module):\n",
    "    def __init__(self, d_model, num_variables, sinusoidal):\n",
    "        super().__init__()\n",
    "        self.sinusoidal = sinusoidal\n",
    "        self.cvs_value = ContinuousValueEmbedding(d_model)\n",
    "        if sinusoidal:\n",
    "            self.cvs_time = SineActivation(1, d_model)\n",
    "        if sinusoidal == \"both\":\n",
    "            self.cvs_time = ContinuousValueEmbedding(d_model)\n",
    "            self.sin_time = SineActivation(1, d_model)\n",
    "        else:\n",
    "            self.cvs_time = ContinuousValueEmbedding(d_model)\n",
    "        self.var_embed = VariableEmbedding(d_model, num_variables)\n",
    "    def forward(self, encoder_input):\n",
    "        time = encoder_input[0]\n",
    "        variable = encoder_input[1]\n",
    "        value = encoder_input[2]\n",
    "        if self.sinusoidal == \"both\":\n",
    "            time_embed = self.cvs_time(time) + self.sin_time(time)\n",
    "        else:\n",
    "            time_embed = self.cvs_time(time)\n",
    "        embed = time_embed + self.cvs_value(value) + self.var_embed(variable)\n",
    "        return embed\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, d_model, d, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.d = d\n",
    "        self.Q = nn.Linear(d_model, d)\n",
    "        self.K = nn.Linear(d_model, d)\n",
    "        self.V = nn.Linear(d_model, d)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self,x, mask): \n",
    "        q = self.Q(x) \n",
    "        k = self.K(x)\n",
    "        v = self.V(x) \n",
    "        weights = q@k.transpose(-2,-1)*k.shape[-1]**(-0.5) \n",
    "        weights = weights.masked_fill(mask == 0, float('-inf'))\n",
    "        weights = F.softmax(weights, dim = -1) \n",
    "        self.dropout(weights)\n",
    "        out = weights @ v\n",
    "        return out \n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, dropout = 0.2):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Attention(d_model, d_model//n_heads) for _ in range(n_heads)])\n",
    "        self.proj = nn.Linear(n_heads*(d_model//n_heads), d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        out = torch.cat([h(x, mask) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "class FeedForwardBlock(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, dropout = 0.2):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.W1 = nn.Linear(d_model, d_ff)\n",
    "        self.W2 = nn.Linear(d_ff, d_model)\n",
    "    def forward(self, x):\n",
    "        out = self.W1(x)\n",
    "        out = F.relu(out)\n",
    "        out = self.dropout(self.W2(out))\n",
    "        return out\n",
    "\n",
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, d_ff):\n",
    "        super().__init__()\n",
    "        self.multi_attention = MultiHeadAttention(d_model, n_heads)\n",
    "        self.ffb = FeedForwardBlock(d_model, d_ff)\n",
    "        self.ln1 = nn.LayerNorm(d_model)\n",
    "        self.ln2 = nn.LayerNorm(d_model)\n",
    "    \n",
    "    def forward(self, x, mask):\n",
    "        out = self.multi_attention(x, mask)\n",
    "        out1 = x + self.ln2(out)\n",
    "        out2 = self.ffb(out1)\n",
    "        out = out1 + self.ln2(out2)\n",
    "        return out\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, d_ff, num_variables , N, sinusoidal):\n",
    "        super().__init__()\n",
    "        self.embedding = Embedding(d_model, num_variables, sinusoidal)\n",
    "        self.encoder_blocks = nn.ModuleList([EncoderBlock(d_model, n_heads, d_ff) for _ in range(N)])\n",
    "        self.N = N\n",
    "    \n",
    "    def forward(self, encoder_input, mask):\n",
    "        time = encoder_input[0]\n",
    "        variable = encoder_input[1]\n",
    "        value = encoder_input[2]\n",
    "        x = self.embedding((time, variable, value))\n",
    "        for block in self.encoder_blocks:\n",
    "            x = block(x, mask)\n",
    "        return x\n",
    "\n",
    "class FusionSelfAttention(nn.Module):\n",
    "    def __init__(self, d_model, dropout = 0.2):\n",
    "        super().__init__()\n",
    "        self.Wa = nn.Linear(d_model, d_model)\n",
    "        self.Ua = nn.Linear(d_model, d_model)\n",
    "        self.Va = nn.Linear(d_model, 1)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        \n",
    "    def forward(self, out, mask):\n",
    "        q = out.unsqueeze(2) \n",
    "        k = out.unsqueeze(1) \n",
    "        v = out \n",
    "        a = F.tanh(self.Wa(q) + self.Ua(k)) \n",
    "        wei = self.Va(self.dropout(a)).squeeze()\n",
    "        wei = wei.masked_fill(mask == 0, float('-inf'))\n",
    "        wei = F.softmax(wei, dim = -1)\n",
    "        wei = self.dropout(wei)\n",
    "        out = wei@v\n",
    "        return out\n",
    "        \n",
    "class Model(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, d_ff, num_variables, N, sinusoidal = False):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(d_model, n_heads, d_ff, num_variables, N, sinusoidal)\n",
    "        self.fsa = FusionSelfAttention(d_model)\n",
    "        self.proj = nn.Linear(d_model, 1)\n",
    "    \n",
    "    def forward(self, x, mask):\n",
    "        out = self.encoder(x, mask)\n",
    "        out = self.fsa(out, mask)\n",
    "        # out = out.masked_fill(mask.transpose(-2,-1)==0, 0)\n",
    "        # out = out.sum(dim = 1)\n",
    "        out = self.proj(out)\n",
    "        return out.squeeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdfe9b82-ebb6-48d5-bcd2-8811ccef1868",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = Model(d_model, num_heads, d_ff, num_variables, N, sinusoidal).to(DEVICE)\n",
    "criterion = nn.MSELoss(reduce=False)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f'Total number of parameters: {total_params}')\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "early_stopping_counter = 0\n",
    "patience = 10 \n",
    "\n",
    "def calculate_loss(model, data_loader):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs in data_loader:\n",
    "            outputs = model(inputs['encoder_input'], inputs['encoder_mask'])\n",
    "            outputs = torch.where(torch.logical_or(pretraining_mask==0,pretraining_mask==-1), torch.tensor(0.0), outputs)\n",
    "            labels = torch.where(torch.logical_or(pretraining_mask==0,pretraining_mask==-1), torch.tensor(0.0), batch['labels'])\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss = torch.where(pretraining_mask==1, loss, torch.zeros_like(loss))\n",
    "            loss = torch.sum(loss)\n",
    "            N = torch.sum(pretraining_mask == 1).item()\n",
    "            loss /= N\n",
    "            total_loss += loss.item()\n",
    "    return total_loss/len(data_loader)\n",
    "\n",
    "for epoch in range(1):\n",
    "    total_loss = 0\n",
    "    model.train()\n",
    "    n = 0\n",
    "    for batch in tqdm(train_dataloader_inhospital, desc=f'Epoch {epoch + 1}/{epochs}', leave=False, mininterval=1):\n",
    "        inp = batch['encoder_input']\n",
    "        mask = batch['encoder_mask']\n",
    "        pretraining_mask = batch['pretraining_mask']\n",
    "        outputs = model(inp, mask)\n",
    "        outputs = torch.where(torch.logical_or(pretraining_mask==0,pretraining_mask==-1), torch.tensor(0.0), outputs)\n",
    "        labels = torch.where(torch.logical_or(pretraining_mask==0,pretraining_mask==-1), torch.tensor(0.0), batch['labels'])\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss = torch.where(pretraining_mask==1, loss, torch.zeros_like(loss))\n",
    "        loss = torch.sum(loss)\n",
    "        N = torch.sum(pretraining_mask == 1).item()\n",
    "        loss /= N\n",
    "        total_loss += loss.item()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        n+=1\n",
    "        if n%500 == 0:\n",
    "            val_loss = calculate_loss(model, val_dataloader_inhospital)\n",
    "            print(f'Epoch {epoch + 1}/{epochs} batches {n}, Validation Loss: {val_loss:.3f}', end='\\r')\n",
    "    val_loss = calculate_loss(model, val_dataloader_inhospital)\n",
    "    print(f'Epoch {epoch + 1}/{epochs}, Training Loss: {total_loss/len(train_dataloader_inhospital):.3f}')\n",
    "    print(f'Epoch {epoch + 1}/{epochs}, Validation Loss: {val_loss:.3f}')\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        early_stopping_counter = 0\n",
    "    else:\n",
    "        early_stopping_counter += 1\n",
    "\n",
    "    if early_stopping_counter >= patience:\n",
    "        print(f\"Early stopping after {epoch + 1} epochs.\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c664a65-533a-451f-ac94-708a2d584004",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for batch in train_dataloader_inhospital:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e17d4da4-287c-40e5-9551-79293b84c0c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch['encoder_input'][2][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc799e1-b08d-482a-a799-545d80253016",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch['pretraining_mask'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a143537a-f7c3-4632-9e98-53ed48281179",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "outputs = model(batch['encoder_input'], batch['encoder_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba16d08a-8b9e-436e-bcf1-0c757148227d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "outputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1762c0e-4f16-40ff-b950-fd41f75da821",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!gpustat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c3fb1ec-e090-4bff-8280-c7a0420ce21a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!CUDA_VISIBLE_DEVICES=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a68ba489-5836-451a-a21f-7152d41b4e41",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mimic",
   "language": "python",
   "name": "mimic"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
