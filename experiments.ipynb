{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3ce172b-a057-4852-acdb-c5376e414d2a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import average_precision_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "def calculate_roc_auc_prc(model, data_loader):\n",
    "    model.eval()\n",
    "    all_probabilities = []\n",
    "    all_labels = []\n",
    "    total_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs in tqdm(data_loader, leave=False):\n",
    "            outputs = model(inputs['encoder_input'], inputs['encoder_mask'])\n",
    "            labels = inputs['label']\n",
    "            logits = torch.sigmoid(outputs)\n",
    "            loss = criterion(outputs, labels.float().view(-1,1))\n",
    "            total_loss += loss.item()\n",
    "            all_probabilities.append(logits.cpu().numpy())\n",
    "            all_labels.append(labels.cpu().numpy())\n",
    "\n",
    "    logits_all = np.concatenate(all_probabilities)\n",
    "    labels_all = np.concatenate(all_labels)\n",
    "    total_loss = total_loss/len(data_loader)\n",
    "    \n",
    "    roc_auc = roc_auc_score(labels_all, logits_all)\n",
    "    auc_prc = average_precision_score(labels_all, logits_all)\n",
    "    return roc_auc, auc_prc, total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "54dcdd8f-b14f-4d9c-b9ce-0bfb41193d5d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "MAX_LEN = 256\n",
    "batch_size = 32\n",
    "d_model = 50\n",
    "num_heads = 4\n",
    "N = 2\n",
    "num_variables = 18 \n",
    "num_variables += 1 #for no variable embedding while doing padding\n",
    "d_ff = 100\n",
    "epochs = 75\n",
    "learning_rate = 1e-5\n",
    "drop_out = 0.2\n",
    "sinusoidal = False\n",
    "th_val_roc = 0.84\n",
    "th_val_pr = 0.48\n",
    "Uniform = True\n",
    "import torch\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "pd.set_option('future.no_silent_downcasting',True)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from tqdm import tqdm\n",
    "from normalizer import Normalizer\n",
    "from categorizer import Categorizer\n",
    "\n",
    "train_data_path_inhospital = \"/data/datasets/mimic3_18var/root/in-hospital-mortality/train_listfile.csv\"\n",
    "val_data_path_inhospital = \"/data/datasets/mimic3_18var/root/in-hospital-mortality/val_listfile.csv\"\n",
    "\n",
    "train_data_path_phenotyping = \"/data/datasets/mimic3_18var/root/phenotyping/train_listfile.csv\"\n",
    "val_data_path_phenotyping = \"/data/datasets/mimic3_18var/root/phenotyping/val_listfile.csv\"\n",
    "\n",
    "train_data_path_decompensation = \"/data/datasets/mimic3_18var/root/decompensation/train_listfile.csv\"\n",
    "val_data_path_decompensation = \"/data/datasets/mimic3_18var/root/decompensation/val_listfile.csv\"\n",
    "\n",
    "data_dir_inhospital = \"/data/datasets/mimic3_18var/root/in-hospital-mortality/train/\"\n",
    "data_dir_phenotyping = \"/data/datasets/mimic3_18var/root/phenotyping/train/\"\n",
    "data_dir_decompensation = \"/data/datasets/mimic3_18var/root/decompensation/train/\"\n",
    "\n",
    "import pickle\n",
    "\n",
    "with open('normalizer.pkl', 'rb') as file:\n",
    "    normalizer = pickle.load(file)\n",
    "\n",
    "with open('categorizer.pkl', 'rb') as file:\n",
    "    categorizer = pickle.load(file)\n",
    "    \n",
    "\n",
    "mean_variance = normalizer.mean_var_dict\n",
    "cat_dict = categorizer.category_dict\n",
    "\n",
    "\n",
    "train_ds_inhospital = MaskedMimicDataSetInHospitalMortality(data_dir_inhospital, train_data_path_inhospital, mean_variance, cat_dict, 'training', MAX_LEN)\n",
    "val_ds_inhospital = MaskedMimicDataSetInHospitalMortality(data_dir_inhospital, val_data_path_inhospital, mean_variance, cat_dict, 'validation', MAX_LEN)\n",
    "\n",
    "train_ds_phenotyping = MaskedMimicDataSetInHospitalMortality(data_dir_phenotyping, train_data_path_phenotyping, mean_variance, cat_dict, 'training', MAX_LEN)\n",
    "val_ds_phenotyping = MaskedMimicDataSetInHospitalMortality(data_dir_phenotyping, val_data_path_phenotyping, mean_variance, cat_dict, 'validation', MAX_LEN)\n",
    "\n",
    "train_ds_decompensation = MaskedMimicDataSetInHospitalMortality(data_dir_decompensation, train_data_path_decompensation, mean_variance, cat_dict, 'training', MAX_LEN)\n",
    "val_ds_decompensation = MaskedMimicDataSetInHospitalMortality(data_dir_decompensation, val_data_path_decompensation, mean_variance, cat_dict, 'validation', MAX_LEN)\n",
    "\n",
    "\n",
    "train_dataloader_inhospital = DataLoader(train_ds_inhospital, batch_size = batch_size, shuffle=True)\n",
    "val_dataloader_inhospital = DataLoader(val_ds_inhospital, batch_size = 1)\n",
    "\n",
    "train_dataloader_phenotyping = DataLoader(train_ds_phenotyping, batch_size = batch_size, shuffle=True)\n",
    "val_dataloader_phenotyping = DataLoader(val_ds_phenotyping, batch_size = 1)\n",
    "\n",
    "train_dataloader_decompensation = DataLoader(train_ds_decompensation, batch_size = batch_size, shuffle=True)\n",
    "val_dataloader_decompensation = DataLoader(val_ds_decompensation, batch_size = 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c3f8f53a-6240-4253-a0ef-deb276127102",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch.nn import functional as F\n",
    "import math\n",
    "\n",
    "def t2v(tau, f, out_features, w, b, w0, b0, arg=None):\n",
    "    if arg:\n",
    "        v1 = f(torch.matmul(tau, w) + b, arg)\n",
    "    else:\n",
    "        v1 = f(torch.matmul(tau, w) + b)\n",
    "    v2 = torch.matmul(tau, w0) + b0\n",
    "    return torch.cat([v1, v2], -1)\n",
    "\n",
    "class SineActivation(nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(SineActivation, self).__init__()\n",
    "        self.out_features = out_features\n",
    "        self.w0 = nn.parameter.Parameter(torch.randn(in_features, 1))\n",
    "        self.b0 = nn.parameter.Parameter(torch.randn(1))\n",
    "        self.w = nn.parameter.Parameter(torch.randn(in_features, out_features-1))\n",
    "        self.b = nn.parameter.Parameter(torch.randn(out_features-1))\n",
    "        self.f = torch.sin\n",
    "\n",
    "    def forward(self, tau):\n",
    "        return t2v(tau.unsqueeze(-1), self.f, self.out_features, self.w, self.b, self.w0, self.b0)\n",
    "\n",
    "class CosineActivation(nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(CosineActivation, self).__init__()\n",
    "        self.out_features = out_features\n",
    "        self.w0 = nn.parameter.Parameter(torch.randn(in_features, 1))\n",
    "        self.b0 = nn.parameter.Parameter(torch.randn(1))\n",
    "        self.w = nn.parameter.Parameter(torch.randn(in_features, out_features-1))\n",
    "        self.b = nn.parameter.Parameter(torch.randn(out_features-1))\n",
    "        self.f = torch.cos\n",
    "\n",
    "    def forward(self, tau):\n",
    "        return t2v(tau.unsqueeze(-1), self.f, self.out_features, self.w, self.b, self.w0, self.b0)\n",
    "\n",
    "\n",
    "class ContinuousValueEmbedding(nn.Module):\n",
    "    def __init__(self, d_model):\n",
    "        super().__init__()\n",
    "        self.W = nn.Linear(1, d_model*2)\n",
    "        self.U = nn.Linear(d_model*2, d_model)\n",
    "        self.tanh = nn.Tanh()\n",
    "    def forward(self, x):\n",
    "        out = self.W(x.unsqueeze(2))\n",
    "        out = self.tanh(out)\n",
    "        out = self.U(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class VariableEmbedding(nn.Module):\n",
    "    def __init__(self, d_model, num_variables):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(num_variables+1, d_model)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.embedding(x)\n",
    "    \n",
    "    \n",
    "\n",
    "class Embedding(nn.Module):\n",
    "    def __init__(self, d_model, num_variables, sinusoidal):\n",
    "        super().__init__()\n",
    "        self.sinusoidal = sinusoidal\n",
    "        self.cvs_value = ContinuousValueEmbedding(d_model)\n",
    "        if sinusoidal:\n",
    "            self.cvs_time = SineActivation(1, d_model)\n",
    "        if sinusoidal == \"both\":\n",
    "            self.cvs_time = ContinuousValueEmbedding(d_model)\n",
    "            self.sin_time = SineActivation(1, d_model)\n",
    "        else:\n",
    "            self.cvs_time = ContinuousValueEmbedding(d_model)\n",
    "        self.var_embed = VariableEmbedding(d_model, num_variables)\n",
    "    def forward(self, encoder_input):\n",
    "        time = encoder_input[0]\n",
    "        variable = encoder_input[1]\n",
    "        value = encoder_input[2]\n",
    "        if self.sinusoidal == \"both\":\n",
    "            time_embed = self.cvs_time(time) + self.sin_time(time)\n",
    "        else:\n",
    "            time_embed = self.cvs_time(time)\n",
    "        embed = time_embed + self.cvs_value(value) + self.var_embed(variable)\n",
    "        return embed\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, d_model, d, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.d = d\n",
    "        self.Q = nn.Linear(d_model, d)\n",
    "        self.K = nn.Linear(d_model, d)\n",
    "        self.V = nn.Linear(d_model, d)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self,x, mask): \n",
    "        q = self.Q(x) \n",
    "        k = self.K(x)\n",
    "        v = self.V(x) \n",
    "        weights = q@k.transpose(-2,-1)*k.shape[-1]**(-0.5) \n",
    "        weights = weights.masked_fill(mask == 0, float('-inf'))\n",
    "        weights = F.softmax(weights, dim = -1) \n",
    "        self.dropout(weights)\n",
    "        out = weights @ v\n",
    "        return out \n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, dropout = 0.2):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Attention(d_model, d_model//n_heads) for _ in range(n_heads)])\n",
    "        self.proj = nn.Linear(n_heads*(d_model//n_heads), d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        out = torch.cat([h(x, mask) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "class FeedForwardBlock(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, dropout = 0.2):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.W1 = nn.Linear(d_model, d_ff)\n",
    "        self.W2 = nn.Linear(d_ff, d_model)\n",
    "    def forward(self, x):\n",
    "        out = self.W1(x)\n",
    "        out = F.relu(out)\n",
    "        out = self.dropout(self.W2(out))\n",
    "        return out\n",
    "\n",
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, d_ff):\n",
    "        super().__init__()\n",
    "        self.multi_attention = MultiHeadAttention(d_model, n_heads)\n",
    "        self.ffb = FeedForwardBlock(d_model, d_ff)\n",
    "        self.ln1 = nn.LayerNorm(d_model)\n",
    "        self.ln2 = nn.LayerNorm(d_model)\n",
    "    \n",
    "    def forward(self, x, mask):\n",
    "        out = self.multi_attention(x, mask)\n",
    "        out1 = x + self.ln2(out)\n",
    "        out2 = self.ffb(out1)\n",
    "        out = out1 + self.ln2(out2)\n",
    "        return out\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, d_ff, num_variables , N, sinusoidal):\n",
    "        super().__init__()\n",
    "        self.embedding = Embedding(d_model, num_variables, sinusoidal)\n",
    "        self.encoder_blocks = nn.ModuleList([EncoderBlock(d_model, n_heads, d_ff) for _ in range(N)])\n",
    "        self.N = N\n",
    "    \n",
    "    def forward(self, encoder_input, mask):\n",
    "        time = encoder_input[0]\n",
    "        variable = encoder_input[1]\n",
    "        value = encoder_input[2]\n",
    "        x = self.embedding((time, variable, value))\n",
    "        for block in self.encoder_blocks:\n",
    "            x = block(x, mask)\n",
    "        return x\n",
    "\n",
    "class FusionSelfAttention(nn.Module):\n",
    "    def __init__(self, d_model, dropout = 0.2):\n",
    "        super().__init__()\n",
    "        self.Wa = nn.Linear(d_model, d_model)\n",
    "        self.Ua = nn.Linear(d_model, d_model)\n",
    "        self.Va = nn.Linear(d_model, 1)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        \n",
    "    def forward(self, out, mask):\n",
    "        q = out.unsqueeze(2) \n",
    "        k = out.unsqueeze(1) \n",
    "        v = out \n",
    "        a = F.tanh(self.Wa(q) + self.Ua(k)) \n",
    "        wei = self.Va(self.dropout(a)).squeeze()\n",
    "        wei = wei.masked_fill(mask == 0, float('-inf'))\n",
    "        wei = F.softmax(wei, dim = -1)\n",
    "        wei = self.dropout(wei)\n",
    "        out = wei@v\n",
    "        return out\n",
    "        \n",
    "class Model(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, d_ff, num_variables, N, sinusoidal = False):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(d_model, n_heads, d_ff, num_variables, N, sinusoidal)\n",
    "        self.fsa = FusionSelfAttention(d_model)\n",
    "        self.proj = nn.Linear(d_model, 1)\n",
    "    \n",
    "    def forward(self, x, mask):\n",
    "        out = self.encoder(x, mask)\n",
    "        out = self.fsa(out, mask)\n",
    "        # out = out.masked_fill(mask.transpose(-2,-1)==0, 0)\n",
    "        # out = out.sum(dim = 1)\n",
    "        out = self.proj(out)\n",
    "        return out.squeeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cdfe9b82-ebb6-48d5-bcd2-8811ccef1868",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 56990\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/75, Training Loss: 0.363\n",
      "Epoch 1/75, Validation Loss: 0.371\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/75, Training Loss: 0.286\n",
      "Epoch 2/75, Validation Loss: 0.271\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/75, Training Loss: 0.263\n",
      "Epoch 3/75, Validation Loss: 0.312\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/75, Training Loss: 0.139\n",
      "Epoch 4/75, Validation Loss: 0.588\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/75, Training Loss: 0.065\n",
      "Epoch 5/75, Validation Loss: 0.588\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/75, Training Loss: 0.058\n",
      "Epoch 6/75, Validation Loss: 0.637\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/75, Training Loss: 0.056\n",
      "Epoch 7/75, Validation Loss: 0.635\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/75, Training Loss: 0.060\n",
      "Epoch 8/75, Validation Loss: 0.543\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/75, Training Loss: 0.051\n",
      "Epoch 9/75, Validation Loss: 0.543\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/75, Training Loss: 0.051\n",
      "Epoch 10/75, Validation Loss: 0.463\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/75, Training Loss: 0.045\n",
      "Epoch 11/75, Validation Loss: 0.596\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/75, Training Loss: 0.043\n",
      "Epoch 12/75, Validation Loss: 0.526\n",
      "Early stopping after 12 epochs.\n"
     ]
    }
   ],
   "source": [
    "model = Model(d_model, num_heads, d_ff, num_variables, N, sinusoidal).to(DEVICE)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f'Total number of parameters: {total_params}')\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "early_stopping_counter = 0\n",
    "patience = 10 \n",
    "\n",
    "def calculate_loss(model, data_loader):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs in data_loader:\n",
    "            outputs = model(inputs['encoder_input'], inputs['encoder_mask'])\n",
    "            outputs = torch.where(torch.logical_or(pretraining_mask==0,pretraining_mask==-1), torch.tensor(0.0), outputs)\n",
    "            labels = torch.where(torch.logical_or(pretraining_mask==0,pretraining_mask==-1), torch.tensor(0.0), batch['labels'])\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "    return total_loss/len(data_loader)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    model.train()\n",
    "    n = 0\n",
    "    for batch in tqdm(train_dataloader_inhospital, desc=f'Epoch {epoch + 1}/{epochs}', leave=False, mininterval=1):\n",
    "        inp = batch['encoder_input']\n",
    "        mask = batch['encoder_mask']\n",
    "        pretraining_mask = batch['pretraining_mask']\n",
    "        outputs = model(inp, mask)\n",
    "        outputs = torch.where(torch.logical_or(pretraining_mask==0,pretraining_mask==-1), torch.tensor(0.0), outputs)\n",
    "        labels = torch.where(torch.logical_or(pretraining_mask==0,pretraining_mask==-1), torch.tensor(0.0), batch['labels'])\n",
    "        loss = criterion(outputs, labels)\n",
    "        total_loss += loss.item()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        n+=1\n",
    "        if n%500 == 0:\n",
    "            val_loss = calculate_loss(model, val_dataloader_inhospital)\n",
    "            print(f'Epoch {epoch + 1}/{epochs} batches {n}, Validation Loss: {val_loss:.3f}', end='\\r')\n",
    "    val_loss = calculate_loss(model, val_dataloader_inhospital)\n",
    "    print(f'Epoch {epoch + 1}/{epochs}, Training Loss: {total_loss/len(train_dataloader_inhospital):.3f}')\n",
    "    print(f'Epoch {epoch + 1}/{epochs}, Validation Loss: {val_loss:.3f}')\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        early_stopping_counter = 0\n",
    "    else:\n",
    "        early_stopping_counter += 1\n",
    "\n",
    "    if early_stopping_counter >= patience:\n",
    "        print(f\"Early stopping after {epoch + 1} epochs.\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c7f72a9-9723-4293-acf9-980cd08a74a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = Model(d_model, num_heads, d_ff, num_variables, N).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e42ba6a-e6a7-4fd0-b7e8-cb12dd7f8c3a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "out = model(batch['encoder_input'], batch['encoder_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c621d8c-93ac-48a5-8f00-e7c3682f93c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35190186-4afc-4938-ba4b-fbbf6482a04c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mask = batch['pretraining_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9583d87-b1bd-4fb8-89b1-aa95c94c4f55",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch['pretraining_mask'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e3bf382-75c8-4e3a-83ef-00427262dc87",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mask = mask.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecdde395-4112-455f-82ea-a2ea62473191",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch['encoder_input'][2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e2e3a7d-c2b7-479b-81df-c76dbb487867",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "out = torch.where(torch.logical_or(mask==0,mask==-1), torch.tensor(0.0), out)\n",
    "# output = torch.where((mask==0) or (mask==-1), torch.tensor(0.0), output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69031430-320a-44bc-99e0-0e6b81978bca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e599d5-167a-410b-997c-df7e1a3d6124",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6cea2c0-31d3-4bed-9dc9-05220f625d86",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.logical_or(mask==0, mask==1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b5259e-2f9e-4ed1-89aa-ba64da0d311a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e2fac7-df42-489f-8148-86fa13b268a0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.rand(100) < 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d39463-588b-41d5-85c8-1786c43d8f25",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def mask()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mimic",
   "language": "python",
   "name": "mimic"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
